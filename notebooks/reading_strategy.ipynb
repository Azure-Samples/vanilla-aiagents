{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Strategies\n",
    "\n",
    "This is advanced feature meant to optimize message handling as the conversation grows. It is not necessary to understand this feature to use the library, but it will likely be required for production cases.\n",
    "\n",
    "## The problems\n",
    "LLMs have limited context windows, which means that they can only remember a certain number of previous messages. This can be a problem when the conversation grows and the model can't handle all of them. Additionally, models may get confused when the conversation is too long. And finally, keeping too much data can easily lead to ineffeciency (especially when running agents remotely)\n",
    "\n",
    "## Solution\n",
    "\n",
    "The solution is to use reading strategies. Reading strategies are a way to tell the model which messages to keep and which to discard. This way, the model can focus on the most relevant messages and ignore the rest. This can be done by implementing a `ReadingStrategy` class and passing it to an `Agent` or `Team`.\n",
    "\n",
    "**NOTE** that the original conversation is NOT modified. The reading strategy is only used to filter the messages that the model sees at any given round.\n",
    "\n",
    "## Available strategies\n",
    "\n",
    "The library provides a few reading strategies that can be used out of the box. They are:\n",
    "\n",
    "- `AllMessagesStrategy`: keeps all messages. Default one.\n",
    "- `LastNMessagesStrategy`: keeps the last N messages.\n",
    "- `TopKLastNMessagesStrategy`: keeps the top K messages and the last N messages.\n",
    "- `SummarizeMessagesStrategy`: returns a single message that summarizes the conversation.\n",
    "- `PipelineConversationReadingStrategy`: allows to chain multiple strategies.\n",
    "\n",
    "## See also\n",
    "\n",
    "Complemenary to reading strategies, there are also _update strategies_ that allow to change how agents update the conversation history. This is useful when you want to keep the conversation history clean and avoid duplicates. See the [update strategies notebook](update_strategies.ipynb) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to sys.path\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join('../vanilla_aiagents')))\n",
    "\n",
    "from vanilla_aiagents.llm import AzureOpenAILLM\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAILLM({\n",
    "    \"azure_deployment\": os.getenv(\"AZURE_OPENAI_MODEL\"),\n",
    "    \"azure_endpoint\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    \"api_version\": os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "})\n",
    "\n",
    "# Set logging to debug for Agent, User and Workflow\n",
    "import logging\n",
    "\n",
    "# Set logging to debug for Agent, User, and Workflow\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"vanilla_aiagents.agent\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"vanilla_aiagents.workflow\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"vanilla_aiagents.team\").setLevel(logging.DEBUG)\n",
    "logging.getLogger(\"vanilla_aiagents.llm\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vanilla_aiagents.conversation import Conversation, SummarizeMessagesStrategy\n",
    "conversation = Conversation(messages=[\n",
    "            {\"role\": \"system\", \"content\": \"\"},\n",
    "            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"Hello! How can I help you today?\"},\n",
    "            {\"role\": \"user\",\"name\": \"user\", \"content\": \"Hi! Can you help me with capital cities?\"},\n",
    "            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"Sure! Which capital you want to learn about?\"},\n",
    "            {\"role\": \"user\",\"name\": \"user\", \"content\": \"Which is the capital of Italy?\"},\n",
    "            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"The capital of Italy is Rome.\"},\n",
    "            {\"role\": \"user\",\"name\": \"user\", \"content\": \"And the capital of France?\"},\n",
    "            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"The capital of France is Paris.\"},\n",
    "            {\"role\": \"user\",\"name\": \"user\", \"content\": \"Thank you!\"},\n",
    "            {\"role\": \"assistant\",\"name\": \"agent\", \"content\": \"You're welcome!\"},\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "strategy = SummarizeMessagesStrategy(llm=llm, system_prompt=\"Summarize the conversation, highlighting the key points in a bullet list\")\n",
    "result = strategy.get_messages(conversation)\n",
    "result[0][\"content\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
